{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPXGlJhdilG/3hSq04I9q9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NarendraKumarMadireddy/Denosing-Dynamic-PET-Images-using-DAE/blob/main/projectPoison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twRKERk9n1F_",
        "outputId": "23baf54b-c118-4c28-bf7a-2125119fbb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel pydicom matplotlib tensorflow scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8xRaUcWoSQh",
        "outputId": "7928e329-b29b-4fed-9846-35f81aae3e52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nibabel\n",
            "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, pydicom, nibabel, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 nibabel-5.3.2 pydicom-3.0.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import nibabel as nib\n",
        "import os\n",
        "import pydicom\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LUuTUOLloFaM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DICOM_FOLDER_PATH = '/content/drive/MyDrive/PT_80p 150_30 OSEM'  # CHANGE THIS PATH\n",
        "\n",
        "# Set where you want to save results\n",
        "RESULTS_FOLDER = '/content/drive/MyDrive/results3'  # CHANGE THIS PATH\n",
        "IMAGE_OUTPUT_FOLDER = os.path.join(RESULTS_FOLDER, 'images')  # Folder for image outputs\n",
        "\n",
        "# Create results folders if they don't exist\n",
        "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
        "os.makedirs(IMAGE_OUTPUT_FOLDER, exist_ok=True)"
      ],
      "metadata": {
        "id": "1JLywuqtpBab"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dicom_files(directory):\n",
        "    \"\"\"Load all DICOM files from a directory structure.\"\"\"\n",
        "    dicom_files = []\n",
        "\n",
        "    # Walk through directory structure\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.dcm'):\n",
        "                dicom_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(dicom_files)} DICOM files\")\n",
        "    return dicom_files"
      ],
      "metadata": {
        "id": "xzh0fg9TpXj1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def organize_pet_series(dicom_files):\n",
        "    \"\"\"Organize DICOM files into different PET series.\"\"\"\n",
        "    series_dict = {}\n",
        "\n",
        "    for file_path in tqdm(dicom_files, desc=\"Reading DICOM metadata\"):\n",
        "        try:\n",
        "            ds = pydicom.dcmread(file_path)\n",
        "\n",
        "            # Skip non-PET images if needed\n",
        "            if hasattr(ds, 'Modality') and ds.Modality != 'PT':\n",
        "                continue\n",
        "\n",
        "            # Use SeriesInstanceUID as the key\n",
        "            series_id = ds.SeriesInstanceUID\n",
        "\n",
        "            if series_id not in series_dict:\n",
        "                series_dict[series_id] = []\n",
        "\n",
        "            series_dict[series_id].append((file_path, ds))\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    print(f\"Found {len(series_dict)} unique PET series\")\n",
        "    return series_dict"
      ],
      "metadata": {
        "id": "mRckPe3MpX2n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_pet_volume(series_files):\n",
        "    \"\"\"Convert a DICOM series into a 3D numpy array.\"\"\"\n",
        "    # Sort files by slice location or instance number\n",
        "    try:\n",
        "        sorted_files = sorted(series_files, key=lambda x: float(x[1].SliceLocation) if hasattr(x[1], 'SliceLocation') else x[1].InstanceNumber)\n",
        "    except:\n",
        "        # Fallback sorting by filename if SliceLocation not available\n",
        "        sorted_files = sorted(series_files, key=lambda x: x[0])\n",
        "\n",
        "    # Get dimensions from first image\n",
        "    ds = sorted_files[0][1]\n",
        "    rows = ds.Rows\n",
        "    cols = ds.Columns\n",
        "\n",
        "    # Initialize volume array\n",
        "    volume = np.zeros((len(sorted_files), rows, cols))\n",
        "\n",
        "    # Fill the volume with pixel data\n",
        "    for i, (_, ds) in enumerate(tqdm(sorted_files, desc=\"Building volume\")):\n",
        "        # Rescale the pixel values if needed\n",
        "        rescale_slope = 1.0\n",
        "        rescale_intercept = 0.0\n",
        "\n",
        "        if hasattr(ds, 'RescaleSlope'):\n",
        "            rescale_slope = float(ds.RescaleSlope)\n",
        "        if hasattr(ds, 'RescaleIntercept'):\n",
        "            rescale_intercept = float(ds.RescaleIntercept)\n",
        "\n",
        "        # Extract pixel array and apply rescaling\n",
        "        pixel_array = ds.pixel_array\n",
        "        volume[i, :, :] = pixel_array * rescale_slope + rescale_intercept\n",
        "\n",
        "    return volume\n",
        "\n",
        "def normalize_volume(volume):\n",
        "    \"\"\"Normalize volume to 0-1 range.\"\"\"\n",
        "    min_val = np.min(volume)\n",
        "    max_val = np.max(volume)\n",
        "\n",
        "    if max_val > min_val:\n",
        "        return (volume - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        return volume"
      ],
      "metadata": {
        "id": "Y9bs_u6upoDZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_poisson_noise(clean_data, snr=10):\n",
        "    \"\"\"Add Poisson noise to simulate low-dose PET acquisition.\"\"\"\n",
        "    # Scale the data\n",
        "    scaled_data = clean_data * snr\n",
        "\n",
        "    # Add Poisson noise\n",
        "    noisy_data = np.random.poisson(scaled_data)\n",
        "\n",
        "    # Scale back to original range\n",
        "    noisy_data = noisy_data / snr\n",
        "\n",
        "    # Clip to [0, 1] for normalized data\n",
        "    noisy_data = np.clip(noisy_data, 0, 1)\n",
        "\n",
        "    return noisy_data"
      ],
      "metadata": {
        "id": "EqSU301epoYz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patches(volume, patch_size=(5, 5, 5), stride=2):\n",
        "    \"\"\"Extract 3D patches from a volume.\"\"\"\n",
        "    # Volume shape: [Z, X, Y]\n",
        "    Z, X, Y = volume.shape\n",
        "\n",
        "    patches = []\n",
        "    positions = []\n",
        "\n",
        "    # Extract patches using sliding window\n",
        "    for z in range(0, Z - patch_size[0] + 1, stride):\n",
        "        for x in range(0, X - patch_size[1] + 1, stride):\n",
        "            for y in range(0, Y - patch_size[2] + 1, stride):\n",
        "                # Extract patch\n",
        "                patch = volume[z:z+patch_size[0],\n",
        "                               x:x+patch_size[1],\n",
        "                               y:y+patch_size[2]]\n",
        "\n",
        "                # Reshape to vector\n",
        "                patch_vector = patch.flatten()\n",
        "\n",
        "                patches.append(patch_vector)\n",
        "                positions.append((z, x, y))\n",
        "\n",
        "    return np.array(patches), positions"
      ],
      "metadata": {
        "id": "sZRT2OqNqAza"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_data(clean_volumes, noisy_volumes, patch_size=(5, 5, 5), stride=2):\n",
        "    \"\"\"Prepare training data by extracting patches from clean and noisy volumes.\"\"\"\n",
        "    all_clean_patches = []\n",
        "    all_noisy_patches = []\n",
        "\n",
        "    for clean_vol, noisy_vol in zip(clean_volumes, noisy_volumes):\n",
        "        clean_patches, _ = extract_patches(clean_vol, patch_size, stride)\n",
        "        noisy_patches, _ = extract_patches(noisy_vol, patch_size, stride)\n",
        "\n",
        "        all_clean_patches.append(clean_patches)\n",
        "        all_noisy_patches.append(noisy_patches)\n",
        "\n",
        "    # Concatenate patches from all volumes\n",
        "    X_train = np.vstack(all_noisy_patches)\n",
        "    y_train = np.vstack(all_clean_patches)\n",
        "\n",
        "    return X_train, y_train"
      ],
      "metadata": {
        "id": "c14YgT8yqIDZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Custom thresholded accuracy metric\n",
        "def thresholded_accuracy(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.cast(tf.abs(y_true - y_pred) <= 0.1, tf.float32))\n",
        "\n",
        "def build_dae_model(input_dim, hidden_dim=128, n_layers=5):\n",
        "    \"\"\"Build a deep autoencoder model for denoising.\"\"\"\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    x = Dense(hidden_dim * 2, activation='elu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Hidden layers\n",
        "    x = Dense(hidden_dim, activation='elu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    for i in range(n_layers - 3):\n",
        "        x = Dense(hidden_dim // 2, activation='elu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "    # Bottleneck layer\n",
        "    x = Dense(hidden_dim // 4, activation='relu')(x)\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(n_layers - 3):\n",
        "        x = Dense(hidden_dim // 2, activation='elu')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "    x = Dense(hidden_dim, activation='elu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Dense(hidden_dim * 2, activation='elu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model with custom accuracy\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mean_squared_error',\n",
        "        metrics=[thresholded_accuracy]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "YD6oBn-ZqPFj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dae(X_train, y_train, model, epochs=100, batch_size=64, validation_split=0.2):\n",
        "    \"\"\"Train the DAE model.\"\"\"\n",
        "    # Define callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    checkpoint = ModelCheckpoint(os.path.join(RESULTS_FOLDER, 'dae_model.h5'),\n",
        "                                monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping, checkpoint],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "PbNdyNyDqYMz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denoise_volume(model, noisy_volume, patch_size=(5, 5, 5), stride=2):\n",
        "    \"\"\"Apply DAE to denoise a full PET volume.\"\"\"\n",
        "    # Get volume shape\n",
        "    Z, X, Y = noisy_volume.shape\n",
        "\n",
        "    # Initialize output volume and count matrix (for averaging overlapping patches)\n",
        "    denoised_volume = np.zeros_like(noisy_volume)\n",
        "    count_matrix = np.zeros_like(noisy_volume)\n",
        "\n",
        "    # Extract patches and their positions\n",
        "    patches, positions = extract_patches(noisy_volume, patch_size, stride)\n",
        "\n",
        "    # Denoise patches in batches to avoid memory issues\n",
        "    batch_size = 1000\n",
        "    n_batches = (patches.shape[0] + batch_size - 1) // batch_size\n",
        "\n",
        "    denoised_patches = []\n",
        "    for i in range(n_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, patches.shape[0])\n",
        "        batch_denoised = model.predict(patches[start_idx:end_idx], verbose=0)\n",
        "        denoised_patches.append(batch_denoised)\n",
        "\n",
        "    denoised_patches = np.vstack(denoised_patches)\n",
        "\n",
        "    # Put denoised patches back into the volume\n",
        "    for i, (z, x, y) in enumerate(positions):\n",
        "        # Reshape denoised patch back to original shape\n",
        "        denoised_patch = denoised_patches[i].reshape(patch_size)\n",
        "\n",
        "        # Add denoised patch to the output volume\n",
        "        denoised_volume[z:z+patch_size[0], x:x+patch_size[1], y:y+patch_size[2]] += denoised_patch\n",
        "\n",
        "        # Update count matrix\n",
        "        count_matrix[z:z+patch_size[0], x:x+patch_size[1], y:y+patch_size[2]] += 1\n",
        "\n",
        "    # Average overlapping patches\n",
        "    denoised_volume = np.divide(denoised_volume, count_matrix, out=np.zeros_like(denoised_volume), where=count_matrix!=0)\n",
        "\n",
        "    return denoised_volume"
      ],
      "metadata": {
        "id": "VQl5LMVHqcyZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_slice_images(original_volume, noisy_volume, denoised_volume, series_idx, metrics=None):\n",
        "    \"\"\"Save individual slice images for comparison.\"\"\"\n",
        "    # Create folder for this series\n",
        "    series_folder = os.path.join(IMAGE_OUTPUT_FOLDER, f'series_{series_idx}')\n",
        "    os.makedirs(series_folder, exist_ok=True)\n",
        "\n",
        "    # Get dimensions\n",
        "    Z, X, Y = original_volume.shape\n",
        "\n",
        "    # Calculate slice positions for visualization (start, middle, end)\n",
        "    slice_positions = [\n",
        "        Z // 4,           # First quarter\n",
        "        Z // 2,           # Middle\n",
        "        3 * Z // 4        # Third quarter\n",
        "    ]\n",
        "\n",
        "    # Save selected slices\n",
        "    for pos_idx, slice_pos in enumerate(slice_positions):\n",
        "        if slice_pos >= Z:\n",
        "            continue\n",
        "\n",
        "        # Save original slice\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(original_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Original Image - Slice {slice_pos}')\n",
        "        plt.savefig(os.path.join(series_folder, f'original_slice_{pos_idx}.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save noisy slice\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(noisy_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        psnr_text = f' - PSNR: {metrics[\"psnr_noisy\"]:.2f} dB' if metrics else ''\n",
        "        plt.title(f'Noisy Image - Slice {slice_pos}{psnr_text}')\n",
        "        plt.savefig(os.path.join(series_folder, f'noisy_slice_{pos_idx}.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save denoised slice\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(denoised_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        psnr_text = f' - PSNR: {metrics[\"psnr_denoised\"]:.2f} dB' if metrics else ''\n",
        "        plt.title(f'Denoised Image - Slice {slice_pos}{psnr_text}')\n",
        "        plt.savefig(os.path.join(series_folder, f'denoised_slice_{pos_idx}.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Save side-by-side comparison\n",
        "        plt.figure(figsize=(18, 6))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(original_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        plt.title(f'Original - Slice {slice_pos}')\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(noisy_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        psnr_text = f' - PSNR: {metrics[\"psnr_noisy\"]:.2f} dB' if metrics else ''\n",
        "        plt.title(f'Noisy{psnr_text}')\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(denoised_volume[slice_pos], cmap='hot')\n",
        "        plt.colorbar()\n",
        "        psnr_text = f' - PSNR: {metrics[\"psnr_denoised\"]:.2f} dB' if metrics else ''\n",
        "        plt.title(f'Denoised{psnr_text}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(series_folder, f'comparison_slice_{pos_idx}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    # Save additional views (axial, coronal, sagittal)\n",
        "    save_multiview_comparison(original_volume, noisy_volume, denoised_volume, series_folder, metrics)"
      ],
      "metadata": {
        "id": "BhemgX94qeNi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_multiview_comparison(original_volume, noisy_volume, denoised_volume, output_folder, metrics=None):\n",
        "    \"\"\"Save comparison of axial, coronal, and sagittal views.\"\"\"\n",
        "    Z, X, Y = original_volume.shape\n",
        "\n",
        "    # Define positions for each view\n",
        "    axial_pos = Z // 2\n",
        "    coronal_pos = X // 2\n",
        "    sagittal_pos = Y // 2\n",
        "\n",
        "    # --------------- Axial View (z-plane) ---------------\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_volume[axial_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original - Axial View')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(noisy_volume[axial_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    psnr_text = f' - PSNR: {metrics[\"psnr_noisy\"]:.2f} dB' if metrics else ''\n",
        "    plt.title(f'Noisy{psnr_text}')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(denoised_volume[axial_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    psnr_text = f' - PSNR: {metrics[\"psnr_denoised\"]:.2f} dB' if metrics else ''\n",
        "    plt.title(f'Denoised{psnr_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'axial_view_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # --------------- Coronal View (x-plane) ---------------\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_volume[:, coronal_pos, :], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original - Coronal View')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(noisy_volume[:, coronal_pos, :], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Noisy{psnr_text}')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(denoised_volume[:, coronal_pos, :], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Denoised{psnr_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'coronal_view_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_volume[:, :, sagittal_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original - Sagittal View')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(noisy_volume[:, :, sagittal_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Noisy{psnr_text}')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(denoised_volume[:, :, sagittal_pos], cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Denoised{psnr_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'sagittal_view_comparison.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "UCzeJU-jqkIr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_mip_comparison(original_volume, noisy_volume, denoised_volume, output_folder, metrics=None):\n",
        "    \"\"\"Save maximum intensity projection (MIP) comparisons.\"\"\"\n",
        "    # Create MIPs\n",
        "    original_mip = np.max(original_volume, axis=0)\n",
        "    noisy_mip = np.max(noisy_volume, axis=0)\n",
        "    denoised_mip = np.max(denoised_volume, axis=0)\n",
        "\n",
        "    # Save comparison\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original - MIP')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(noisy_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    psnr_text = f' - PSNR: {metrics[\"psnr_noisy\"]:.2f} dB' if metrics else ''\n",
        "    plt.title(f'Noisy{psnr_text}')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(denoised_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    psnr_text = f' - PSNR: {metrics[\"psnr_denoised\"]:.2f} dB' if metrics else ''\n",
        "    plt.title(f'Denoised{psnr_text}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'mip_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Save individual MIPs at higher resolution\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(original_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original - Maximum Intensity Projection')\n",
        "    plt.savefig(os.path.join(output_folder, 'original_mip.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(noisy_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Noisy - Maximum Intensity Projection{psnr_text}')\n",
        "    plt.savefig(os.path.join(output_folder, 'noisy_mip.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(denoised_mip, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Denoised - Maximum Intensity Projection{psnr_text}')\n",
        "    plt.savefig(os.path.join(output_folder, 'denoised_mip.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "QiA0PTO8qqLi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_difference_images(original_volume, noisy_volume, denoised_volume, output_folder):\n",
        "    \"\"\"Save difference images to show noise patterns removed.\"\"\"\n",
        "    # Calculate difference volumes\n",
        "    noisy_diff = np.abs(noisy_volume - original_volume)\n",
        "    denoised_diff = np.abs(denoised_volume - original_volume)\n",
        "    improvement = noisy_diff - denoised_diff  # Positive values show improvement\n",
        "\n",
        "    # Middle slice\n",
        "    middle_slice = original_volume.shape[0] // 2\n",
        "\n",
        "    # Save difference images\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(noisy_diff[middle_slice], cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original vs Noisy Difference')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(denoised_diff[middle_slice], cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original vs Denoised Difference')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(improvement[middle_slice], cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "    plt.colorbar()\n",
        "    plt.title('Improvement (Red = Better)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'difference_analysis.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Save MIP of difference volumes\n",
        "    noisy_diff_mip = np.max(noisy_diff, axis=0)\n",
        "    denoised_diff_mip = np.max(denoised_diff, axis=0)\n",
        "    improvement_mip = np.max(improvement, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(noisy_diff_mip, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original vs Noisy Difference - MIP')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(denoised_diff_mip, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Original vs Denoised Difference - MIP')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(improvement_mip, cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
        "    plt.colorbar()\n",
        "    plt.title('Improvement - MIP (Red = Better)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_folder, 'difference_analysis_mip.png'))\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "zHLxkafVquZs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading DICOM files...\")\n",
        "dicom_files = load_dicom_files(DICOM_FOLDER_PATH)\n",
        "\n",
        "if len(dicom_files) == 0:\n",
        "    print(f\"No DICOM files found in {DICOM_FOLDER_PATH}\")\n",
        "else:\n",
        "    # Organize into series\n",
        "    series_dict = organize_pet_series(dicom_files)\n",
        "\n",
        "    if len(series_dict) == 0:\n",
        "        print(\"No valid PET series found\")\n",
        "    else:\n",
        "        # Build volumes from the first few series for training\n",
        "        volumes = []\n",
        "        series_ids = list(series_dict.keys())\n",
        "\n",
        "        # Limit to maximum 5 series for training to avoid memory issues\n",
        "        max_training_series = min(5, len(series_ids))\n",
        "\n",
        "        print(f\"Building volumes from {max_training_series} series for training...\")\n",
        "        for i in range(max_training_series):\n",
        "            series_id = series_ids[i]\n",
        "            print(f\"Processing series {i+1}/{max_training_series}: {series_id[:8]}...\")\n",
        "            volume = build_pet_volume(series_dict[series_id])\n",
        "            volumes.append(normalize_volume(volume))\n",
        "\n",
        "        # Create noisy versions for training\n",
        "        noisy_volumes = []\n",
        "        clean_volumes = []\n",
        "\n",
        "        print(\"Generating noisy training data...\")\n",
        "        for vol in volumes:\n",
        "            # Use the original as clean\n",
        "            clean_volumes.append(vol)\n",
        "\n",
        "            # Add noise for training\n",
        "            noisy_vol = add_poisson_noise(vol, snr=5)\n",
        "            noisy_volumes.append(noisy_vol)\n",
        "\n",
        "        # Prepare training data\n",
        "        print(\"Preparing training patches...\")\n",
        "        patch_size = (5, 5, 5)  # 3D patch size\n",
        "        stride = 2              # Stride for patch extraction\n",
        "\n",
        "        X_train, y_train = prepare_training_data(clean_volumes, noisy_volumes, patch_size, stride)\n",
        "        print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
        "\n",
        "        # Build and train model\n",
        "        print(\"Building DAE model...\")\n",
        "        input_dim = X_train.shape[1]\n",
        "        model = build_dae_model(input_dim, hidden_dim=128, n_layers=5)\n",
        "        model.summary()\n",
        "\n",
        "        print(\"Training DAE model...\")\n",
        "        model, history = train_dae(X_train, y_train, model, epochs=3, batch_size=128)\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training vs Validation Loss')\n",
        "        plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['thresholded_accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_thresholded_accuracy'], label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Training vs Validation Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(RESULTS_FOLDER, 'training_history_with_accuracy.png'))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        # Process all series for denoising\n",
        "        print(\"\\nDenoising all PET series...\")\n",
        "        for i, series_id in enumerate(series_ids):\n",
        "            print(f\"Denoising series {i+1}/{len(series_ids)}: {series_id[:8]}...\")\n",
        "\n",
        "            # Build original volume\n",
        "            original_volume = build_pet_volume(series_dict[series_id])\n",
        "            original_volume_norm = normalize_volume(original_volume)\n",
        "\n",
        "            # Add noise for testing (simulate low-dose)\n",
        "            noisy_volume = add_poisson_noise(original_volume_norm, snr=5)\n",
        "\n",
        "            # Denoise volume\n",
        "            denoised_volume = denoise_volume(model, noisy_volume, patch_size, stride)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse_noisy = np.mean((noisy_volume - original_volume_norm)**2)\n",
        "            mse_denoised = np.mean((denoised_volume - original_volume_norm)**2)\n",
        "\n",
        "            psnr_noisy = 10 * np.log10(1.0 / mse_noisy)  # Normalized to 0-1\n",
        "            psnr_denoised = 10 * np.log10(1.0 / mse_denoised)\n",
        "\n",
        "            metrics = {\n",
        "                'mse_noisy': mse_noisy,\n",
        "                'mse_denoised': mse_denoised,\n",
        "                'psnr_noisy': psnr_noisy,\n",
        "                'psnr_denoised': psnr_denoised\n",
        "            }\n",
        "\n",
        "            print(f\"MSE - Noisy: {mse_noisy:.4f}, Denoised: {mse_denoised:.4f}\")\n",
        "            print(f\"PSNR - Noisy: {psnr_noisy:.4f} dB, Denoised: {psnr_denoised:.4f} dB\")\n",
        "\n",
        "            # Save detailed visualizations\n",
        "            save_slice_images(original_volume_norm, noisy_volume, denoised_volume, i, metrics)\n",
        "\n",
        "            # Save MIP comparisons\n",
        "            series_folder = os.path.join(IMAGE_OUTPUT_FOLDER, f'series_{i}')\n",
        "            save_mip_comparison(original_volume_norm, noisy_volume, denoised_volume, series_folder, metrics)\n",
        "\n",
        "            # Save difference images\n",
        "            save_difference_images(original_volume_norm, noisy_volume, denoised_volume, series_folder)\n",
        "\n",
        "            np.save(os.path.join(RESULTS_FOLDER, f'original_volume_series_{i}.npy'), original_volume_norm)\n",
        "            np.save(os.path.join(RESULTS_FOLDER, f'noisy_volume_series_{i}.npy'), noisy_volume)\n",
        "            np.save(os.path.join(RESULTS_FOLDER, f'denoised_volume_series_{i}.npy'), denoised_volume)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xbMgs7Raq36B",
        "outputId": "38055ae1-3258-4436-d287-acc4c5166c0e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DICOM files...\n",
            "Found 1706 DICOM files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading DICOM metadata: 100%|██████████| 1706/1706 [00:37<00:00, 45.99it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 unique PET series\n",
            "Building volumes from 1 series for training...\n",
            "Processing series 1/1: 1.2.840....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building volume: 100%|██████████| 1706/1706 [00:01<00:00, 1509.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating noisy training data...\n",
            "Preparing training patches...\n",
            "Training data shape: (7519436, 125), (7519436, 125)\n",
            "Building DAE model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m32,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m)            │        \u001b[38;5;34m32,125\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,125</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m163,485\u001b[0m (638.61 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">163,485</span> (638.61 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,437\u001b[0m (630.61 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,437</span> (630.61 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DAE model...\n",
            "Epoch 1/3\n",
            "\u001b[1m46993/46997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - thresholded_accuracy: 0.9821"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m46997/46997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 9ms/step - loss: 0.0026 - thresholded_accuracy: 0.9821 - val_loss: 1.3934e-05 - val_thresholded_accuracy: 0.9999\n",
            "Epoch 2/3\n",
            "\u001b[1m46997/46997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 9ms/step - loss: 3.0865e-05 - thresholded_accuracy: 0.9995 - val_loss: 1.9187e-05 - val_thresholded_accuracy: 0.9999\n",
            "Epoch 3/3\n",
            "\u001b[1m46991/46997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5633e-05 - thresholded_accuracy: 0.9996"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m46997/46997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 9ms/step - loss: 2.5633e-05 - thresholded_accuracy: 0.9996 - val_loss: 7.5792e-06 - val_thresholded_accuracy: 0.9999\n",
            "\n",
            "Denoising all PET series...\n",
            "Denoising series 1/1: 1.2.840....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building volume: 100%|██████████| 1706/1706 [00:00<00:00, 5150.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE - Noisy: 0.0007, Denoised: 0.0000\n",
            "PSNR - Noisy: 31.7896 dB, Denoised: 48.4536 dB\n"
          ]
        }
      ]
    }
  ]
}